# -*- coding: utf-8 -*-
"""Codebase_RAG_Completed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/team-headstart/CodebaseRAG/blob/main/Codebase_RAG_Completed.ipynb

# Install Necessary Libraries
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from langchain_pinecone import PineconeVectorStore
from langchain.embeddings import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from pinecone import Pinecone
import os
from dotenv import load_dotenv
import tempfile
from github import Github, Repository
from git import Repo
from openai import OpenAI
from pathlib import Path
from langchain.schema import Document
from pinecone import Pinecone

load_dotenv()

"""# Clone a GitHub Repo locally"""
from sentence_transformers import SentenceTransformer
from langchain_pinecone import PineconeVectorStore
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from dotenv import load_dotenv
from git import Repo
import os
from pinecone import Pinecone
from openai import OpenAI

# Load environment variables
load_dotenv()

# Supported file extensions and ignored directories
SUPPORTED_EXTENSIONS = {'.py', '.js', '.tsx', '.jsx', '.ipynb', '.java',
                        '.cpp', '.ts', '.go', '.rs', '.vue', '.swift', '.c', '.h'}
IGNORED_DIRS = {'node_modules', 'venv', 'env', 'dist', 'build', '.git',
                '__pycache__', '.next', '.vscode', 'vendor'}

# Initialize Pinecone
# pinecone_api_key = os.getenv("PINECONE_API_KEY")
# pinecone_index_name = "codebase-rag"
# pinecone_client = Pinecone(api_key=pinecone_api_key)
# pinecone_index = pinecone_client.Index(pinecone_index_name)


def get_huggingface_embeddings(text, model_name="sentence-transformers/all-mpnet-base-v2"):
    """Generates embeddings using a HuggingFace model."""
    model = SentenceTransformer(model_name)
    return model.encode(text)


def clone_repository(repo_url):
    """Clones a GitHub repository to the local 'content' directory."""
    repo_name = repo_url.split("/")[-1]
    repo_path = f"./content/{repo_name}"

    # Skip cloning if the repo already exists
    if os.path.exists(repo_path):
        print(f"Repository already exists at: {repo_path}")
        return repo_path

    print(f"Cloning repository from {repo_url} to {repo_path}...")
    Repo.clone_from(repo_url, repo_path)
    print(f"Cloned repository to: {repo_path}")
    return repo_path


def get_file_content(file_path, repo_path):
    """Reads and returns the content of a single file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        rel_path = os.path.relpath(file_path, repo_path)
        return {"name": rel_path, "content": content}
    except Exception as e:
        print(f"Error reading file {file_path}: {e}")
        return None


pinecone_api_key = os.getenv("PINECONE_API_KEY")
# Initialize Pinecone
pc = Pinecone(api_key=pinecone_api_key,)

# Connect to your Pinecone index
pinecone_index = pc.Index("codebase-rag")



def get_main_files_content(repo_path):
    """Gets the content of supported files from the given repository."""
    files_content = []

    for root, _, files in os.walk(repo_path):
        # Skip ignored directories
        if any(ignored_dir in root for ignored_dir in IGNORED_DIRS):
            continue

        for file in files:
            file_path = os.path.join(root, file)
            if os.path.splitext(file)[1] in SUPPORTED_EXTENSIONS:
                file_content = get_file_content(file_path, repo_path)
                if file_content:
                    files_content.append(file_content)
    return files_content


def add_repo_to_pinecone(repo_url):
    """Adds a GitHub repository to Pinecone and local content directory."""
    # Clone the repository
    repo_path = clone_repository(repo_url)
    repo_namespace = repo_url  # Using the repo URL as the namespace in Pinecone

    # Get the files' content from the repository
    files_content = get_main_files_content(repo_path)
    if not files_content:
        print(f"No valid files found in repository: {repo_url}")
        return

    # Prepare documents for Pinecone
    documents = []
    for file in files_content:
        doc = Document(
            page_content=f"{file['name']}\n{file['content']}",
            metadata={"source": file['name']}
        )

        documents.append(doc)
    pinecone_index = pc.Index("codebase-rag")
    # Add documents to Pinecone
    vectorstore = PineconeVectorStore(
        index_name="codebase-rag",
        embedding=HuggingFaceEmbeddings()
    )
    vectorstore.from_documents(documents=documents, embedding=HuggingFaceEmbeddings(), index_name='codebase-rag', namespace=repo_namespace)
    print(f"Repository added to Pinecone under namespace: {repo_namespace}")
    return repo_namespace


"""# Putting it all together"""

groq_api = os.getenv("GROQ_API_KEY")
client = OpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key=groq_api
)

def perform_rag(query):
    raw_query_embedding = get_huggingface_embeddings(query)

    top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=5, include_metadata=True, namespace="https://github.com/CoderAgent/SecureAgent")

    # Get the list of retrieved texts
    contexts = [item['metadata']['text'] for item in top_matches['matches']]

    augmented_query = "<CONTEXT>\n" + "\n\n-------\n\n".join(contexts[ : 10]) + "\n-------\n</CONTEXT>\n\n\n\nMY QUESTION:\n" + query

    # Modify the prompt below as need to improve the response quality
    system_prompt = f"""You are a Senior Software Engineer, specializing in TypeScript.

    Answer any questions I have about the codebase, based on the code provided. Always consider all of the context provided when forming a response.
    """

    llm_response = client.chat.completions.create(
        model="llama-3.1-8b-instant",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": augmented_query}
        ]
    )

    return llm_response.choices[0].message.content

# response = perform_rag("How is the javascript parser used?")

# print(response)





def perform_rag_with_namespace(query, namespace):
    """
    Performs retrieval-augmented generation (RAG) using Pinecone and OpenAI, allowing namespace specification.

    Args:
        query (str): The query to be answered.
        namespace (str): The Pinecone namespace to query.

    Returns:
        str: The generated response from the language model.
    """
    # Generate query embeddings
    raw_query_embedding = get_huggingface_embeddings(query)

    try:
        # Query the Pinecone index using the specified namespace
        top_matches = pinecone_index.query(
            vector=raw_query_embedding.tolist(),
            top_k=5,  # Retrieve top 5 results
            include_metadata=True,
            namespace=namespace  # Specify the namespace
        )
    except Exception as e:
        raise ValueError(f"Error querying Pinecone: {e}")

    # Extract context from matches
    contexts = [item['metadata']['text'] for item in top_matches['matches']]

    # Construct the augmented query with context
    augmented_query = "<CONTEXT>\n" + "\n\n-------\n\n".join(contexts[:10]) + "\n-------\n</CONTEXT>\n\nMY QUESTION:\n" + query

    # Define the system prompt
    system_prompt = f"""You are an Expert Senior Software Engineer, specializing in TypeScript.
    Answer any questions I have about the codebase, based on the code provided. Always consider all of the context provided when forming a response and give as many code examples as possible in your response.
    DO NOT HALLUCINATE. Lets think about this step by step, and answer it step by step.
    """

    # Query the language model
    try:
        llm_response = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": augmented_query}
            ]
        )
    except Exception as e:
        raise ValueError(f"Error querying language model: {e}")

    # Return the response content
    return llm_response.choices[0].message.content
